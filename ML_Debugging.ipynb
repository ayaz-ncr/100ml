{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Debugging is Hard.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaz-ncr/100ml/blob/master/ML_Debugging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JndnmDMp66FL"
      },
      "source": [
        "#### Copyright 2018 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "hMqWDc_m6rUC",
        "colab": {}
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91QkRICDEYqT"
      },
      "source": [
        "# Counterintuitive Challenges in ML Debugging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9vC0sXBEEgUF"
      },
      "source": [
        "In this Colab, you will explore why ML debugging is harder than traditional debugging by debugging a simple regression problem, with one feature and one label. You will:\n",
        "\n",
        "* Create the dataset.\n",
        "* Try to fit the data with a simple model.\n",
        "* Debug the model.\n",
        "* Demonstrate exploding gradients.\n",
        "\n",
        "Please **make a copy** of this Colab before running it. Click on *File*, and then click on *Save a copy in Drive*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5IPfZryiJJXv"
      },
      "source": [
        "# Case Study: Debugging a Simple Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O-vpfBAN48gW"
      },
      "source": [
        "## Create the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_OeRWVAGvF63"
      },
      "source": [
        "Run the cells below to load libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SYj-8T48e6Rw",
        "colab": {}
      },
      "source": [
        "# Reset environment for a new run\n",
        "% reset -f\n",
        "\n",
        "# Load Libraries\n",
        "from os.path import join # for joining file pathnames\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set Pandas display options\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ox6Jlt_rj8s0"
      },
      "source": [
        "Create the data. Your data consists of one feature with values 0 to 9, and your labels are the same data with some noise added. In a dataset, by convention, rows are examples and columns are features. To match this convention, transpose your data. Before transposing your vectors, you must convert them to matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4aN24LlKj7LM",
        "colab": {}
      },
      "source": [
        "features = np.array(range(10))\n",
        "features = features[:, np.newaxis]\n",
        "# Create labels by adding noise distributed around 0\n",
        "labels = features + np.random.random(size=[10,1]) - 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bMXvcL7fpkG2"
      },
      "source": [
        "Verify that the data roughly lies in a straight line and, therefore, is easily predicted..\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P6fomFA9pnrF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "491a07c1-7149-4ff9-b5eb-68695f272aac"
      },
      "source": [
        "# Visualize the data\n",
        "plt.scatter(features,labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f0b4c58a890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MAVm0L6GCwrs"
      },
      "source": [
        "## Fit Simple Data with Simple Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vgn_hESwkhmd"
      },
      "source": [
        "TensorFlow provides several different APIs. This Colab only demonstrates the Keras API since Keras lets you quickly train models in a few lines of code using high-level APIs. In Keras, the typical neural network is a `sequential` model with fully-connected, or `dense`, layers.\n",
        "\n",
        "Your dataset is simple. A neural network with just 1 neuron should learn your dataset. Define a neural network with 1 layer having 1 neuron using the model type `keras.Sequential` with the layer type `keras.layers.Dense`. To understand the Keras code, read the code comments. Then run the cell. The code prints the model summary to show a model with 1 layer and 2 parameters (weight and bias)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E2SHLw83z4fF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6d1041e9-917b-4ddc-bf7e-96448776bf66"
      },
      "source": [
        "# Delete any existing assignment to \"model\"\n",
        "model = None\n",
        "\n",
        "# Use a sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Add a layer with 1 neuron. Use the popular \"tanh\" activation function\n",
        "model.add(keras.layers.Dense(units=1,             # 1 neuron\n",
        "                             activation='tanh',   # 'tanh'\n",
        "                             input_dim=1))         # number of feature cols=1\n",
        "\n",
        "# Model calculates loss using mean-square error (MSE)\n",
        "# Model trains using Adam optimizer with learning rate = 0.001\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
        "              loss='mse',\n",
        "             )\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 10:52:18.665858 139687012591488 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERliMzkpvm8n"
      },
      "source": [
        "Now, train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ldasx-XNvr53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "7aa0cf2f-c647-4e9b-a344-ce520f172231"
      },
      "source": [
        "model.fit(x=features,\n",
        "          y=labels,\n",
        "          epochs=10,    # train for 10 epochs\n",
        "          batch_size=10,# use 10 examples per batch\n",
        "          verbose=1)    # verbose=1 prints progress per epoch"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 0s 48ms/sample - loss: 20.6766\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 453us/sample - loss: 20.6765\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 145us/sample - loss: 20.6764\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 278us/sample - loss: 20.6764\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 226us/sample - loss: 20.6763\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 304us/sample - loss: 20.6762\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 147us/sample - loss: 20.6761\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 291us/sample - loss: 20.6760\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 306us/sample - loss: 20.6759\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 200us/sample - loss: 20.6759\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b4c58a750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CMcdO6GPnBCK"
      },
      "source": [
        "Your loss stubbornly refuses to decrease! Review your approach keeping in mind the guidance on the [model development process](https://developers.google.com/machine-learning/testing-debugging/common/overview). \n",
        "\n",
        "The following list describes possible actions to debug your model. Read the actions and their explanations to understand how debugging in ML requires you to sort through multiple possibilities at once. If an action sounds promising, experiment by modifying the code above.\n",
        "\n",
        "* **Transforming data**: You data is not transformed. You can experiment by transforming the data appropriately and retraining the model.\n",
        "* **Activation function**: The `tanh` activation function cannot predict values >1. Besides, in a regression problem, the last layer should always use the linear activation function. Therefore, should you use  `activation='linear'`?\n",
        "* **Hyperparameter values**: Should you adjust any hyperparameter values to try reducing loss?\n",
        "* **Simpler model**: The model development process recommends starting with a simple model. A linear model is simpler than your nonlinear model. Should you use `activation='linear'`?\n",
        "* **Change optimizer**: Your model uses the Adam optimizer. You can fall back to the gradient descent optimizer by using `optimizer=keras.optimizers.SGD()`.\n",
        "\n",
        "Consider these actions and experiment where necessary. Then read the following section for the solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ej14ORiDUIM3"
      },
      "source": [
        "## Solution: Getting Loss to Decrease"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfGdTkrdUJcm"
      },
      "source": [
        "Before trying to adjust specific model parameters, such as the hyperparameter values, you should first check for good development practices. Here, you should start with a linear model because of these two best practices:\n",
        "\n",
        "* Regression: In a regression problem, the last layer must always be linear.\n",
        "* Start simple: Since a linear model is simpler than a nonliner model, start with a linear model.\n",
        "\n",
        "Run the following code to train a linear model and check if your loss decreases. The code displays the loss curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8fKzl07oWjcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "outputId": "aa27c978-cb9b-4e08-b4c5-4cae009c80c5"
      },
      "source": [
        "model = None\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(1, activation='linear', input_dim=1))\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='mse')\n",
        "trainHistory = model.fit(features, labels, epochs=10, batch_size=1, verbose=1)\n",
        "# Plot loss curve\n",
        "plt.plot(trainHistory.history['loss'])\n",
        "plt.title('Loss Curves')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 0s 8ms/sample - loss: 76.5491\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 75.7992\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 74.9016\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 74.2609\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 73.3957\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 72.5828\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 71.8370\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 71.0769\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 70.3898\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 69.5653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Loss Curves')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXUQkSPwf1hb8QKqix3EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CkcKovAAk4r_"
      },
      "source": [
        "Your loss decreases, albeit slowly! You're on the right track. How can you get your loss to converge? Experiment with the code above. For the solution, read the following section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eXckvj-FlEzl"
      },
      "source": [
        "## Solution: Reaching Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DgBZXOuClIeX"
      },
      "source": [
        "Your loss isn't decreasing fast enough. From the guidance on [Learning Rate](https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate), you know that you can increase the learning rate to train faster. Run the following code to increase the learning rate to 0.1. The the model reaches convergence quickly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRkbxeLVlZoy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "d2b48b20-f0da-41fe-de6e-400cc77a87a9"
      },
      "source": [
        "model = None\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(1, activation='linear', input_dim=1))\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(0.1), loss='mse')\n",
        "model.fit(features, labels, epochs=5, batch_size=1, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "10/10 [==============================] - 0s 10ms/sample - loss: 157.5876\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 66.7834\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 20.1482\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 3.7443\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 0s 1ms/sample - loss: 2.0813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0b2b79ddd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u7Mk-ivUgf9o"
      },
      "source": [
        "Wonderful, you quickly get a very low loss! Let's confirm the model works by predicting results for values [0,9] and superimposing them on top of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WRgXYbBst0Pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "79ab3e6c-e86b-4647-ecbe-ca3d3ad5ed0f"
      },
      "source": [
        "# get predictions\n",
        "featuresPred = model.predict(features, verbose=1)\n",
        "# Plot original features and predicted values\n",
        "featuresPred = np.transpose(featuresPred)\n",
        "plt.scatter(range(10), labels, c=\"blue\")\n",
        "plt.scatter(range(10), featuresPred, c=\"red\")\n",
        "plt.legend([\"Original\", \"Predicted\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r10/10 [==============================] - 0s 3ms/sample\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0b2b5fee10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuAEgYf4f/A4KA5oQF1EAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8s8gb4rTTei7"
      },
      "source": [
        "Yes, the predictions match the features very well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "38cbAf2RpHtI"
      },
      "source": [
        "## Summary of Case Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AjNDlFMMMsTc"
      },
      "source": [
        "When debugging ML models, you should first attempt to diagnose the problem and apply the appropriate fix. For example, if you had changed your optimizer using `optimizer='sgd'`, then your model also converges faster. However, the problem was not with the optimizer but with the learning rate. Changing the optimizer only helps because `optimizer='sgd'` has a higher default learning rate than `optimizer='adam'`.\n",
        "\n",
        "Alternatively, you could train the model for longer with the default learning rate. However, in real-world ML, models take long to train. You should keep your training cycles as short as possible. Therefore, increasing the learning rate is the correct fix.\n",
        "\n",
        "These options demonstrate how debugging in ML is n-dimensional, and therefore you must use your understanding of model mechanics to narrow down your options. Because running experiments in ML is time consuming, requires careful setup, and can be subject to reproducibility issues, it's important to use your understanding of model mechanics to  narrow down options without having to experiment.\n",
        "\n",
        "Lastly, according to development best practices, you should transform your feature data appropriately. This Colab did not transform the feature data because transformation is not required for convergence. However, you should always transform data appropriately. Here, you could normalize your feature data using z-score or scale the feature data to [0,1]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEGEERnbglN9"
      },
      "source": [
        "# Exploding Gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s92MiwHIgm58"
      },
      "source": [
        "A common problem in model training is a loss that \"explodes\" or becomes `nan`. A common cause is anomalous feature data, such as outliers and `nan` values, or a high learning rate. The following sections demonstrate these causes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOg62A4KiLk3"
      },
      "source": [
        "## Cause: High Learning Rate\n",
        "\n",
        "In this section, you will create data in the range [0,50] and show that the gradient explodes when you train the model using a learning rate of 0.01. Then you'll reduce the learning rate to make the model converge.\n",
        "\n",
        "Create and visualize the data by running the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "826oEnhXOi2O",
        "colab": {}
      },
      "source": [
        "# create data with large values\n",
        "features = np.array(range(50))\n",
        "# generate labels\n",
        "labels = features + np.random.random(features.shape) - 0.5\n",
        "\n",
        "# Transpose data for input\n",
        "[features, labels] = [features.transpose(), labels.transpose()]\n",
        "\n",
        "plt.scatter(range(len(features)), features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ckm1y8fCZ1s0"
      },
      "source": [
        "Run the following cell to train a model with a learning rate of 0.01. You will get `inf` for your loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xE8LTD1CZy98",
        "colab": {}
      },
      "source": [
        "# Train on raw data\n",
        "model = None\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Dense(1, input_dim=1, activation='linear'))\n",
        "model.compile(optimizer=keras.optimizers.SGD(0.01), loss='mse')\n",
        "model.fit(features, labels, epochs=5, batch_size=10, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A9QgTarmdWxu"
      },
      "source": [
        "To demonstrate that the high learning rate makes the loss explore, reduce the learning rate to `0.001`. Your loss will converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RWRyRZaXt_l9"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4NmLFsc8Gz67"
      },
      "source": [
        "This Colab demonstrated the following principles.\n",
        "\n",
        "* The n-dimensional nature of debugging in ML makes ML debugging hard.\n",
        "* For effective debugging, understanding model mechanics is important.\n",
        "* Start with a simple model.\n",
        "* Exploding gradients incorrect normalization in the model, mis-configuration of FeatureColumns, etc., than raw data containing NaNs."
      ]
    }
  ]
}