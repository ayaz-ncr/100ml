{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxILBM9rkzXSZV6rixfcEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaz-ncr/100ml/blob/master/iva_bert_multiclass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1bRBYlMk4yp",
        "outputId": "d0fb1be1-610f-46cd-b738-27a25be307a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctXv7s6BkJvd",
        "outputId": "8a12ebb3-496f-4c44-9345-e97ad69579cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"hello world\"\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "output = model(**encoded_input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_m7ZItWlPpa",
        "outputId": "e24a286f-fa79-418a-bbf2-09fa844c8092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH3Bz2PemVze",
        "outputId": "960f60f6-1ddf-484d-c519-39a0d9c7c76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2T7HCN7mV2X",
        "outputId": "8ba3d004-ee92-413a-fe95-afea912eb7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tom_rRvImV5k",
        "outputId": "ca8aaa36-eef2-43fb-cec8-c26378ab634c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1689,  0.1361, -0.1394,  ..., -0.6251,  0.0522,  0.3671],\n",
              "         [-0.3633,  0.1412,  0.8800,  ...,  0.1043,  0.2888,  0.3727],\n",
              "         [-0.6986, -0.6988,  0.0645,  ..., -0.2210,  0.0099, -0.5940],\n",
              "         [ 0.8310,  0.1237, -0.1512,  ...,  0.1031, -0.6779, -0.2629]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.0615e-01, -3.1115e-01, -6.2166e-01,  7.7409e-01,  2.8987e-01,\n",
              "         -1.9024e-01,  9.2471e-01,  1.8180e-01, -5.0504e-01, -9.9994e-01,\n",
              "         -2.3735e-01,  8.7621e-01,  9.7716e-01,  2.5809e-01,  9.3802e-01,\n",
              "         -6.9061e-01, -5.2105e-01, -5.4915e-01,  3.5759e-01, -7.5982e-01,\n",
              "          6.0571e-01,  9.9936e-01,  3.3454e-01,  2.5202e-01,  4.1586e-01,\n",
              "          9.7050e-01, -7.8426e-01,  9.3400e-01,  9.6314e-01,  6.5928e-01,\n",
              "         -7.1543e-01,  1.0239e-01, -9.8550e-01, -1.6261e-01, -6.7724e-01,\n",
              "         -9.8579e-01,  3.0743e-01, -7.8618e-01,  1.3032e-01,  1.2667e-02,\n",
              "         -8.9823e-01,  2.1339e-01,  9.9973e-01, -1.8709e-01,  2.3900e-01,\n",
              "         -2.4464e-01, -1.0000e+00,  2.8436e-01, -8.7582e-01,  7.1257e-01,\n",
              "          6.5697e-01,  5.2143e-01,  1.0539e-01,  4.5140e-01,  4.4865e-01,\n",
              "          2.7456e-01, -9.8213e-02,  9.2734e-02, -2.2094e-01, -5.4374e-01,\n",
              "         -6.0034e-01,  3.6505e-01, -6.5587e-01, -9.1238e-01,  7.8441e-01,\n",
              "          4.8336e-01, -1.0489e-01, -1.9499e-01, -4.8225e-02, -2.2996e-01,\n",
              "          8.9400e-01,  2.5066e-01,  3.8609e-01, -8.4989e-01,  3.2252e-01,\n",
              "          2.1331e-01, -5.5507e-01,  1.0000e+00, -6.5867e-01, -9.7461e-01,\n",
              "          4.9343e-01,  4.6558e-01,  4.7178e-01,  5.5721e-02,  5.1436e-02,\n",
              "         -1.0000e+00,  4.4387e-01, -7.4709e-02, -9.8641e-01,  1.6191e-01,\n",
              "          4.3324e-01, -1.6432e-01, -1.2176e-01,  4.6759e-01, -4.1206e-01,\n",
              "         -2.6535e-01, -3.0527e-01, -5.9662e-01, -2.3193e-01, -1.7307e-01,\n",
              "          1.9924e-02, -2.1107e-01, -2.4200e-01, -3.0866e-01,  2.2780e-01,\n",
              "         -4.2308e-01, -6.4542e-01,  2.6983e-01, -2.7557e-01,  6.3427e-01,\n",
              "          2.9822e-01, -3.1062e-01,  4.6449e-01, -9.6291e-01,  5.5464e-01,\n",
              "         -1.9653e-01, -9.8220e-01, -5.1419e-01, -9.8544e-01,  6.4684e-01,\n",
              "         -1.8899e-01, -2.4919e-01,  9.6180e-01,  3.8441e-01,  2.7922e-01,\n",
              "          5.9582e-02, -6.3763e-01, -1.0000e+00, -6.1973e-01, -4.0413e-01,\n",
              "          4.7591e-02, -1.9830e-01, -9.7161e-01, -9.4822e-01,  5.7882e-01,\n",
              "          9.5076e-01,  9.9642e-02,  9.9916e-01, -2.4947e-01,  9.3731e-01,\n",
              "         -2.0715e-01, -5.7406e-01,  4.2461e-01, -4.3113e-01,  6.4491e-01,\n",
              "          4.8150e-01, -7.2302e-01,  1.0609e-01, -2.4968e-02,  3.2980e-01,\n",
              "         -5.0799e-01, -1.4917e-01, -4.5892e-01, -9.3619e-01, -3.1903e-01,\n",
              "          9.4843e-01, -2.9770e-01, -7.2527e-01,  2.2589e-01, -1.5897e-01,\n",
              "         -4.9687e-01,  8.6441e-01,  6.2593e-01,  2.7812e-01, -2.0720e-01,\n",
              "          3.9699e-01,  6.0161e-02,  5.5732e-01, -8.5815e-01, -1.8942e-02,\n",
              "          3.6645e-01, -2.3459e-01, -4.7094e-01, -9.7639e-01, -2.8612e-01,\n",
              "          4.0802e-01,  9.8942e-01,  7.3076e-01,  2.1438e-01,  5.9475e-01,\n",
              "         -1.3386e-01,  6.8417e-01, -9.4795e-01,  9.7213e-01, -2.9630e-01,\n",
              "          2.2191e-01,  3.1566e-01,  1.7508e-01, -8.5796e-01, -1.5590e-01,\n",
              "          8.6525e-01, -5.4981e-01, -8.6927e-01,  6.0578e-02, -4.4714e-01,\n",
              "         -3.6194e-01, -4.5069e-01,  4.6971e-01, -2.8228e-01, -2.6582e-01,\n",
              "         -2.3449e-02,  8.9092e-01,  9.8083e-01,  8.0408e-01, -2.4909e-02,\n",
              "          6.8485e-01, -9.0679e-01, -5.1557e-01,  5.2714e-02,  2.0982e-01,\n",
              "          1.7816e-01,  9.9394e-01, -3.0574e-01, -1.3667e-01, -9.3650e-01,\n",
              "         -9.8554e-01, -7.2891e-02, -9.1284e-01, -1.0905e-01, -6.3631e-01,\n",
              "          5.1113e-01,  3.8887e-01,  3.1968e-01,  4.0400e-01, -9.9328e-01,\n",
              "         -7.5452e-01,  2.7712e-01, -2.9502e-01,  3.3304e-01, -1.4978e-01,\n",
              "          1.2110e-01,  8.1930e-01, -4.7735e-01,  8.3838e-01,  8.9178e-01,\n",
              "         -4.5513e-01, -7.1493e-01,  8.7837e-01, -2.5635e-01,  8.8714e-01,\n",
              "         -5.9652e-01,  9.7529e-01,  7.9861e-01,  8.0444e-01, -9.1706e-01,\n",
              "         -3.1900e-01, -8.6507e-01, -4.6482e-01,  3.9683e-03, -2.7766e-01,\n",
              "          7.3487e-01,  4.5703e-01,  3.6156e-01,  6.1086e-01, -6.5924e-01,\n",
              "          9.9842e-01, -5.0612e-02, -9.5142e-01,  2.1892e-01, -1.3333e-01,\n",
              "         -9.8295e-01,  6.1005e-01,  2.8506e-01, -2.5937e-01, -4.0388e-01,\n",
              "         -5.4129e-01, -9.5388e-01,  9.2988e-01,  6.1911e-02,  9.8972e-01,\n",
              "          1.4620e-01, -9.4861e-01, -5.4303e-01, -9.0342e-01, -3.3039e-01,\n",
              "         -1.8301e-01, -3.6898e-02, -1.8631e-01, -9.5726e-01,  4.4430e-01,\n",
              "          4.2535e-01,  4.4355e-01, -3.3298e-01,  9.9867e-01,  1.0000e+00,\n",
              "          9.6086e-01,  8.8581e-01,  9.2377e-01, -9.9609e-01, -2.0700e-01,\n",
              "          9.9997e-01, -9.7150e-01, -1.0000e+00, -9.3351e-01, -6.6484e-01,\n",
              "          2.6196e-01, -1.0000e+00, -1.0932e-01,  1.7542e-01, -9.0441e-01,\n",
              "          2.9690e-01,  9.7393e-01,  9.9416e-01, -1.0000e+00,  7.8004e-01,\n",
              "          9.2378e-01, -5.6055e-01,  9.1517e-01, -2.9476e-01,  9.7083e-01,\n",
              "          4.5818e-01,  1.3447e-01, -2.2377e-01,  2.9927e-01, -7.4908e-01,\n",
              "         -8.5180e-01, -2.7195e-01, -4.1478e-01,  9.7242e-01,  8.6409e-02,\n",
              "         -7.5659e-01, -9.2212e-01,  4.7759e-02, -1.4992e-01, -2.6219e-01,\n",
              "         -9.6075e-01, -7.2093e-02,  4.1063e-01,  7.6588e-01,  3.9011e-02,\n",
              "          2.2797e-01, -7.3560e-01,  2.2872e-01, -4.5123e-01,  2.9779e-01,\n",
              "          5.9624e-01, -9.1918e-01, -5.8835e-01, -2.4553e-01, -4.3462e-01,\n",
              "         -2.3733e-01, -9.4708e-01,  9.6731e-01, -4.0816e-01,  7.5546e-01,\n",
              "          1.0000e+00, -2.8027e-02, -8.9186e-01,  5.5393e-01,  1.9125e-01,\n",
              "          8.0895e-02,  1.0000e+00,  7.3035e-01, -9.7404e-01, -4.7621e-01,\n",
              "          4.3255e-01, -4.7478e-01, -4.8012e-01,  9.9834e-01, -2.2458e-01,\n",
              "         -3.4077e-01,  8.3356e-02,  9.6681e-01, -9.8517e-01,  9.5143e-01,\n",
              "         -9.1083e-01, -9.6692e-01,  9.6374e-01,  9.3090e-01, -5.4608e-01,\n",
              "         -5.6511e-01,  7.7313e-02, -3.7943e-01,  1.8914e-01, -9.6841e-01,\n",
              "          7.6807e-01,  4.6560e-01, -4.7643e-02,  8.7445e-01, -8.9385e-01,\n",
              "         -4.4835e-01,  3.3142e-01, -5.3819e-01,  2.0126e-03,  7.1536e-01,\n",
              "          5.2621e-01, -2.8496e-01,  7.2499e-02, -3.1469e-01,  1.6666e-01,\n",
              "         -9.7549e-01,  2.1509e-01,  1.0000e+00, -5.0041e-02,  2.7363e-01,\n",
              "         -3.6835e-01,  1.4367e-02, -2.5385e-01,  4.0477e-01,  4.9355e-01,\n",
              "         -2.7157e-01, -7.9949e-01,  5.5637e-01, -9.7226e-01, -9.8250e-01,\n",
              "          8.2037e-01,  1.2428e-01, -3.1637e-01,  9.9997e-01,  4.8696e-01,\n",
              "          1.6217e-01,  2.2428e-01,  9.4464e-01, -3.0774e-02,  6.7377e-01,\n",
              "          6.1458e-01,  9.7161e-01, -1.8549e-01,  4.9363e-01,  8.6980e-01,\n",
              "         -6.6415e-01, -2.6812e-01, -6.0823e-01, -3.6964e-02, -9.0551e-01,\n",
              "          6.5654e-02, -9.5478e-01,  9.5614e-01,  7.7533e-01,  3.0881e-01,\n",
              "          1.5152e-01,  4.4797e-01,  1.0000e+00,  1.6804e-01,  6.5989e-01,\n",
              "         -6.3028e-01,  9.1261e-01, -9.9569e-01, -8.5203e-01, -3.3441e-01,\n",
              "         -1.2074e-03, -4.8369e-01, -2.5182e-01,  3.0024e-01, -9.6606e-01,\n",
              "          5.3538e-01,  3.7405e-01, -9.9356e-01, -9.9030e-01, -7.9587e-03,\n",
              "          8.9948e-01, -7.5731e-02, -9.0574e-01, -7.3163e-01, -5.7100e-01,\n",
              "          5.1156e-01, -1.4841e-01, -9.4797e-01,  7.2865e-02, -2.1514e-01,\n",
              "          4.5960e-01, -1.0345e-01,  5.0734e-01,  5.2529e-01,  7.6803e-01,\n",
              "         -6.1285e-02,  4.1260e-03, -2.1855e-02, -8.3398e-01,  8.4847e-01,\n",
              "         -8.2300e-01, -7.1640e-01, -1.4696e-01,  1.0000e+00, -4.8598e-01,\n",
              "          6.1346e-01,  7.6874e-01,  7.8394e-01, -4.5107e-02,  9.1026e-02,\n",
              "          7.6333e-01,  1.9754e-01, -4.3364e-01, -5.3293e-01, -8.5295e-01,\n",
              "         -3.3515e-01,  6.7042e-01, -1.3623e-01,  2.7169e-01,  7.9317e-01,\n",
              "          5.5210e-01,  8.3412e-02,  7.5391e-02, -7.1832e-02,  9.9966e-01,\n",
              "         -2.3658e-01, -1.0216e-01, -4.5080e-01,  8.5616e-02, -2.9959e-01,\n",
              "         -6.2786e-01,  1.0000e+00,  3.0930e-01,  1.4167e-01, -9.8768e-01,\n",
              "         -6.8458e-01, -9.1811e-01,  9.9998e-01,  8.2221e-01, -7.2770e-01,\n",
              "          6.7117e-01,  6.6115e-01, -7.5145e-02,  8.6758e-01, -1.3457e-01,\n",
              "         -2.4349e-01,  2.4777e-01,  1.0144e-01,  9.5255e-01, -4.7443e-01,\n",
              "         -9.5957e-01, -5.9315e-01,  3.3709e-01, -9.5565e-01,  9.9735e-01,\n",
              "         -4.4125e-01, -1.3977e-01, -3.6701e-01,  2.0946e-01,  7.4472e-01,\n",
              "         -6.9467e-02, -9.8080e-01, -9.7222e-02,  1.4486e-01,  9.5879e-01,\n",
              "          1.9391e-01, -4.9824e-01, -8.9137e-01,  5.8015e-01,  5.5770e-01,\n",
              "         -6.8508e-01, -9.4907e-01,  9.6072e-01, -9.8609e-01,  5.9676e-01,\n",
              "          1.0000e+00,  2.4973e-01, -3.7395e-01,  1.0501e-01, -4.4424e-01,\n",
              "          2.6123e-01, -2.3078e-01,  7.1235e-01, -9.5670e-01, -3.1889e-01,\n",
              "         -1.6605e-01,  3.2875e-01, -1.1205e-01, -3.6974e-03,  7.0291e-01,\n",
              "          1.7684e-01, -4.3112e-01, -5.8868e-01,  1.9773e-02,  3.7040e-01,\n",
              "          8.4115e-01, -2.7978e-01, -9.0355e-02,  8.1528e-02, -9.3174e-02,\n",
              "         -9.2058e-01, -1.9644e-01, -3.7062e-01, -9.9973e-01,  6.2505e-01,\n",
              "         -1.0000e+00,  1.0621e-01, -1.2206e-01, -1.6786e-01,  8.4365e-01,\n",
              "         -3.7916e-02,  3.5062e-01, -7.8014e-01, -5.3737e-01,  7.0508e-01,\n",
              "          7.8913e-01, -2.8130e-01, -4.3953e-01, -7.0514e-01,  2.4180e-01,\n",
              "         -6.6092e-03,  1.8702e-01, -4.4242e-01,  7.7169e-01, -1.5995e-01,\n",
              "          1.0000e+00,  7.2733e-02, -6.9268e-01, -9.8023e-01,  1.4577e-01,\n",
              "         -2.5391e-01,  1.0000e+00, -9.2031e-01, -9.4661e-01,  3.1159e-01,\n",
              "         -6.4689e-01, -8.4363e-01,  2.5795e-01, -4.2897e-02, -7.7705e-01,\n",
              "         -8.4594e-01,  9.5362e-01,  9.2489e-01, -4.9389e-01,  3.9911e-01,\n",
              "         -3.0106e-01, -4.7879e-01,  7.6661e-02,  5.6425e-01,  9.8501e-01,\n",
              "          1.3876e-01,  8.8985e-01,  5.5478e-01, -5.1430e-02,  9.6024e-01,\n",
              "          1.8294e-01,  5.6980e-01,  4.4824e-02,  1.0000e+00,  2.8503e-01,\n",
              "         -9.1839e-01,  2.6753e-01, -9.7828e-01, -1.6402e-01, -9.6440e-01,\n",
              "          2.8901e-01,  1.6055e-01,  8.8193e-01, -2.2969e-01,  9.5947e-01,\n",
              "         -3.7722e-01,  7.0633e-02, -6.0583e-01, -4.4725e-02,  3.6969e-01,\n",
              "         -8.9272e-01, -9.7725e-01, -9.8374e-01,  5.7585e-01, -4.3288e-01,\n",
              "          3.8856e-04,  1.3452e-01,  8.8691e-02,  2.9999e-01,  4.2328e-01,\n",
              "         -1.0000e+00,  9.3814e-01,  4.0994e-01,  7.0178e-01,  9.5528e-01,\n",
              "          5.5357e-01,  4.0091e-01,  2.3186e-01, -9.7927e-01, -9.8584e-01,\n",
              "         -2.8673e-01, -2.4337e-01,  7.2493e-01,  5.7315e-01,  8.8239e-01,\n",
              "          4.4722e-01, -4.8819e-01,  1.0828e-01, -1.5405e-02, -1.9398e-01,\n",
              "         -9.9173e-01,  4.4130e-01, -3.4468e-01, -9.7585e-01,  9.5458e-01,\n",
              "         -3.4396e-01, -7.9580e-02,  2.5765e-01, -4.4630e-01,  9.6260e-01,\n",
              "          7.0019e-01,  4.7741e-01,  4.4010e-02,  4.4377e-01,  8.6974e-01,\n",
              "          9.5424e-01,  9.8352e-01, -4.9369e-01,  7.9218e-01, -3.5132e-01,\n",
              "          3.8625e-01,  3.8565e-01, -9.2863e-01,  7.0785e-02,  1.6214e-01,\n",
              "         -3.1588e-01,  2.0408e-01, -1.4592e-01, -9.8176e-01,  4.5655e-01,\n",
              "         -2.5013e-01,  5.3519e-01, -3.0179e-01,  1.2085e-01, -3.8113e-01,\n",
              "         -1.1308e-01, -6.9466e-01, -7.3338e-01,  5.4423e-01,  4.4366e-01,\n",
              "          8.8669e-01,  6.6931e-01, -5.6722e-02, -7.1292e-01, -2.4256e-01,\n",
              "         -4.3071e-01, -9.1745e-01,  9.4306e-01, -9.8463e-03, -3.4555e-02,\n",
              "          2.2737e-01, -1.4741e-01,  5.2788e-01, -1.6728e-01, -3.8034e-01,\n",
              "         -3.0925e-01, -6.6327e-01,  8.3470e-01, -4.2989e-02, -4.6003e-01,\n",
              "         -5.9524e-01,  6.2858e-01,  2.4777e-01,  9.9939e-01, -4.3522e-01,\n",
              "         -7.0336e-01, -1.4562e-01, -3.0957e-01,  2.7874e-01, -4.0991e-01,\n",
              "         -1.0000e+00,  3.9205e-01, -1.8185e-01,  4.2810e-01, -5.4864e-01,\n",
              "          3.9862e-01, -5.0946e-01, -9.8203e-01, -1.8075e-01,  7.7715e-02,\n",
              "          4.6108e-01, -4.8077e-01, -6.7799e-01,  4.4749e-01, -4.3145e-02,\n",
              "          8.9809e-01,  8.6203e-01, -1.4951e-01,  4.1630e-01,  5.6036e-01,\n",
              "         -3.0575e-01, -6.4009e-01,  9.1662e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUOMHE4mmvi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tensorflow"
      ],
      "metadata": {
        "id": "cOvZVCZEnE7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"hello world\"\n",
        "encoded_input = tokenizer(text, return_tensors='tf')\n",
        "output = model(encoded_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm1TwNE7mwCo",
        "outputId": "6fd174bb-645c-43c3-c5f1-898d1e3ad6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyK0ptnimwGN",
        "outputId": "39ee8b24-92b4-4584-bd94-47ca6666c33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggrabXsjmwJT",
        "outputId": "22224b8f-eecc-4b7a-d304-86a9f0a68780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.models.bert.modeling_tf_bert.TFBertModel at 0x7fb3f95eed00>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntpxfuyGmwMH",
        "outputId": "b74e1c4d-4c9c-4548-e496-288353ee17ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[ 101, 7592, 2088,  102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 4), dtype=int32, numpy=array([[1, 1, 1, 1]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# {'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ],
      "metadata": {
        "id": "Iel2xrRqnWBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mt1yAqgrmwO-",
        "outputId": "8dff7722-d53c-4c4b-f80e-91a9efbfe134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 4, 768), dtype=float32, numpy=\n",
              "array([[[-0.1688832 ,  0.13606364, -0.1394003 , ..., -0.6251122 ,\n",
              "          0.0521724 ,  0.3671451 ],\n",
              "        [-0.36327484,  0.14121893,  0.87998796, ...,  0.1043295 ,\n",
              "          0.28875726,  0.37267983],\n",
              "        [-0.69859385, -0.6987982 ,  0.06450253, ..., -0.22103664,\n",
              "          0.00986981, -0.593979  ],\n",
              "        [ 0.83098334,  0.12366699, -0.15119109, ...,  0.10309625,\n",
              "         -0.67792654, -0.26285216]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              "array([[-9.06153381e-01, -3.11153024e-01, -6.21653020e-01,\n",
              "         7.74093091e-01,  2.89865583e-01, -1.90243348e-01,\n",
              "         9.24707890e-01,  1.81800842e-01, -5.05039036e-01,\n",
              "        -9.99944508e-01, -2.37347722e-01,  8.76211941e-01,\n",
              "         9.77160394e-01,  2.58089662e-01,  9.38015103e-01,\n",
              "        -6.90611601e-01, -5.21051943e-01, -5.49152911e-01,\n",
              "         3.57588768e-01, -7.59822667e-01,  6.05711102e-01,\n",
              "         9.99358535e-01,  3.34544212e-01,  2.52020419e-01,\n",
              "         4.15862560e-01,  9.70500827e-01, -7.84264266e-01,\n",
              "         9.34000909e-01,  9.63140666e-01,  6.59283817e-01,\n",
              "        -7.15430140e-01,  1.02393322e-01, -9.85498428e-01,\n",
              "        -1.62613422e-01, -6.77240789e-01, -9.85793769e-01,\n",
              "         3.07426661e-01, -7.86176980e-01,  1.30318299e-01,\n",
              "         1.26672937e-02, -8.98226559e-01,  2.13394135e-01,\n",
              "         9.99732614e-01, -1.87095553e-01,  2.38997087e-01,\n",
              "        -2.44638249e-01, -9.99999523e-01,  2.84363627e-01,\n",
              "        -8.75819147e-01,  7.12568462e-01,  6.56972051e-01,\n",
              "         5.21429539e-01,  1.05389774e-01,  4.51396912e-01,\n",
              "         4.48652923e-01,  2.74558932e-01, -9.82132927e-02,\n",
              "         9.27335098e-02, -2.20937833e-01, -5.43738723e-01,\n",
              "        -6.00340009e-01,  3.65051359e-01, -6.55871749e-01,\n",
              "        -9.12379324e-01,  7.84406066e-01,  4.83360887e-01,\n",
              "        -1.04891673e-01, -1.94986418e-01, -4.82243635e-02,\n",
              "        -2.29961887e-01,  8.94001126e-01,  2.50659406e-01,\n",
              "         3.86093557e-01, -8.49888086e-01,  3.22516561e-01,\n",
              "         2.13313460e-01, -5.55065453e-01,  1.00000000e+00,\n",
              "        -6.58669472e-01, -9.74610925e-01,  4.93426830e-01,\n",
              "         4.65575159e-01,  4.71775502e-01,  5.57235256e-02,\n",
              "         5.14330305e-02, -1.00000000e+00,  4.43866640e-01,\n",
              "        -7.47088119e-02, -9.86410201e-01,  1.61914691e-01,\n",
              "         4.33236599e-01, -1.64319694e-01, -1.21764116e-01,\n",
              "         4.67591494e-01, -4.12057310e-01, -2.65350133e-01,\n",
              "        -3.05273622e-01, -5.96615255e-01, -2.31932119e-01,\n",
              "        -1.73068002e-01,  1.99233238e-02, -2.11070001e-01,\n",
              "        -2.41998777e-01, -3.08656514e-01,  2.27797583e-01,\n",
              "        -4.23082262e-01, -6.45423651e-01,  2.69828916e-01,\n",
              "        -2.75569320e-01,  6.34269714e-01,  2.98223674e-01,\n",
              "        -3.10615301e-01,  4.64486092e-01, -9.62906122e-01,\n",
              "         5.54639757e-01, -1.96531802e-01, -9.82197881e-01,\n",
              "        -5.14186084e-01, -9.85441208e-01,  6.46838903e-01,\n",
              "        -1.88990116e-01, -2.49189660e-01,  9.61797416e-01,\n",
              "         3.84414524e-01,  2.79215366e-01,  5.95820881e-02,\n",
              "        -6.37623310e-01, -1.00000000e+00, -6.19731724e-01,\n",
              "        -4.04132307e-01,  4.75920625e-02, -1.98304266e-01,\n",
              "        -9.71608996e-01, -9.48215306e-01,  5.78821421e-01,\n",
              "         9.50758100e-01,  9.96415317e-02,  9.99155164e-01,\n",
              "        -2.49473438e-01,  9.37312961e-01, -2.07149610e-01,\n",
              "        -5.74057341e-01,  4.24611747e-01, -4.31124985e-01,\n",
              "         6.44905210e-01,  4.81503516e-01, -7.23023355e-01,\n",
              "         1.06088579e-01, -2.49670558e-02,  3.29798967e-01,\n",
              "        -5.07989585e-01, -1.49169490e-01, -4.58912820e-01,\n",
              "        -9.36186194e-01, -3.19025904e-01,  9.48431492e-01,\n",
              "        -2.97696233e-01, -7.25267291e-01,  2.25894734e-01,\n",
              "        -1.58971518e-01, -4.96872991e-01,  8.64409328e-01,\n",
              "         6.25930965e-01,  2.78118312e-01, -2.07201108e-01,\n",
              "         3.96993101e-01,  6.01586401e-02,  5.57315588e-01,\n",
              "        -8.58150542e-01, -1.89399999e-02,  3.66453171e-01,\n",
              "        -2.34590665e-01, -4.70936209e-01, -9.76389050e-01,\n",
              "        -2.86121458e-01,  4.08015639e-01,  9.89420056e-01,\n",
              "         7.30756342e-01,  2.14383721e-01,  5.94747543e-01,\n",
              "        -1.33854926e-01,  6.84166253e-01, -9.47948873e-01,\n",
              "         9.72130775e-01, -2.96299368e-01,  2.21911237e-01,\n",
              "         3.15666586e-01,  1.75080687e-01, -8.57959688e-01,\n",
              "        -1.55905217e-01,  8.65249097e-01, -5.49811304e-01,\n",
              "        -8.69274735e-01,  6.05783723e-02, -4.47135061e-01,\n",
              "        -3.61935169e-01, -4.50690269e-01,  4.69705015e-01,\n",
              "        -2.82281548e-01, -2.65824288e-01, -2.34489944e-02,\n",
              "         8.90916288e-01,  9.80835021e-01,  8.04079950e-01,\n",
              "        -2.49125678e-02,  6.84848487e-01, -9.06794488e-01,\n",
              "        -5.15569091e-01,  5.27131371e-02,  2.09819943e-01,\n",
              "         1.78160295e-01,  9.93938088e-01, -3.05737436e-01,\n",
              "        -1.36669174e-01, -9.36495066e-01, -9.85544801e-01,\n",
              "        -7.28914440e-02, -9.12842512e-01, -1.09053448e-01,\n",
              "        -6.36313200e-01,  5.11128485e-01,  3.88871700e-01,\n",
              "         3.19675297e-01,  4.04000014e-01, -9.93275166e-01,\n",
              "        -7.54524946e-01,  2.77123928e-01, -2.95014977e-01,\n",
              "         3.33040178e-01, -1.49775371e-01,  1.21099979e-01,\n",
              "         8.19296002e-01, -4.77350295e-01,  8.38380635e-01,\n",
              "         8.91778588e-01, -4.55126166e-01, -7.14932323e-01,\n",
              "         8.78365278e-01, -2.56351769e-01,  8.87136877e-01,\n",
              "        -5.96521974e-01,  9.75291848e-01,  7.98604965e-01,\n",
              "         8.04440379e-01, -9.17056143e-01, -3.18993628e-01,\n",
              "        -8.65065575e-01, -4.64821965e-01,  3.96851916e-03,\n",
              "        -2.77665079e-01,  7.34863639e-01,  4.57031071e-01,\n",
              "         3.61555517e-01,  6.10862672e-01, -6.59242034e-01,\n",
              "         9.98418391e-01, -5.06106913e-02, -9.51422393e-01,\n",
              "         2.18926340e-01, -1.33329168e-01, -9.82953489e-01,\n",
              "         6.10046685e-01,  2.85061598e-01, -2.59373963e-01,\n",
              "        -4.03874815e-01, -5.41292548e-01, -9.53878939e-01,\n",
              "         9.29880023e-01,  6.19103238e-02,  9.89715934e-01,\n",
              "         1.46200731e-01, -9.48611736e-01, -5.43024302e-01,\n",
              "        -9.03423727e-01, -3.30394328e-01, -1.83007613e-01,\n",
              "        -3.68944332e-02, -1.86305970e-01, -9.57259059e-01,\n",
              "         4.44296986e-01,  4.25347626e-01,  4.43553627e-01,\n",
              "        -3.32978368e-01,  9.98670697e-01,  9.99997020e-01,\n",
              "         9.60860431e-01,  8.85805368e-01,  9.23773885e-01,\n",
              "        -9.96093035e-01, -2.07000747e-01,  9.99970615e-01,\n",
              "        -9.71499383e-01, -1.00000000e+00, -9.33505297e-01,\n",
              "        -6.64835930e-01,  2.61962384e-01, -1.00000000e+00,\n",
              "        -1.09317318e-01,  1.75416887e-01, -9.04407501e-01,\n",
              "         2.96900868e-01,  9.73925769e-01,  9.94156003e-01,\n",
              "        -1.00000000e+00,  7.80043304e-01,  9.23778594e-01,\n",
              "        -5.60553968e-01,  9.15168762e-01, -2.94755459e-01,\n",
              "         9.70825315e-01,  4.58180308e-01,  1.34470001e-01,\n",
              "        -2.23771706e-01,  2.99273789e-01, -7.49081910e-01,\n",
              "        -8.51798534e-01, -2.71950930e-01, -4.14777339e-01,\n",
              "         9.72414553e-01,  8.64083692e-02, -7.56592810e-01,\n",
              "        -9.22118306e-01,  4.77571972e-02, -1.49914920e-01,\n",
              "        -2.62187839e-01, -9.60754216e-01, -7.20926523e-02,\n",
              "         4.10622746e-01,  7.65878618e-01,  3.90103534e-02,\n",
              "         2.27966920e-01, -7.35595405e-01,  2.28718698e-01,\n",
              "        -4.51228440e-01,  2.97789514e-01,  5.96239388e-01,\n",
              "        -9.19182658e-01, -5.88352740e-01, -2.45524853e-01,\n",
              "        -4.34617460e-01, -2.37325132e-01, -9.47083890e-01,\n",
              "         9.67306197e-01, -4.08162564e-01,  7.55454302e-01,\n",
              "         1.00000000e+00, -2.80287359e-02, -8.91859412e-01,\n",
              "         5.53924620e-01,  1.91252083e-01,  8.08957219e-02,\n",
              "         1.00000000e+00,  7.30344355e-01, -9.74041402e-01,\n",
              "        -4.76210892e-01,  4.32550848e-01, -4.74775255e-01,\n",
              "        -4.80114162e-01,  9.98341024e-01, -2.24578083e-01,\n",
              "        -3.40771854e-01,  8.33581686e-02,  9.66808796e-01,\n",
              "        -9.85167086e-01,  9.51426327e-01, -9.10832107e-01,\n",
              "        -9.66916382e-01,  9.63740706e-01,  9.30904388e-01,\n",
              "        -5.46078742e-01, -5.65105259e-01,  7.73122311e-02,\n",
              "        -3.79423380e-01,  1.89137951e-01, -9.68407273e-01,\n",
              "         7.68068731e-01,  4.65599984e-01, -4.76432778e-02,\n",
              "         8.74454319e-01, -8.93845916e-01, -4.48351353e-01,\n",
              "         3.31419259e-01, -5.38185894e-01,  2.01467145e-03,\n",
              "         7.15355933e-01,  5.26208639e-01, -2.84962445e-01,\n",
              "         7.24980533e-02, -3.14690441e-01,  1.66662768e-01,\n",
              "        -9.75485682e-01,  2.15090632e-01,  1.00000000e+00,\n",
              "        -5.00391647e-02,  2.73621976e-01, -3.68351460e-01,\n",
              "         1.43671716e-02, -2.53850520e-01,  4.04768080e-01,\n",
              "         4.93544817e-01, -2.71571010e-01, -7.99486578e-01,\n",
              "         5.56370735e-01, -9.72263873e-01, -9.82498825e-01,\n",
              "         8.20372164e-01,  1.24281317e-01, -3.16365182e-01,\n",
              "         9.99972641e-01,  4.86960024e-01,  1.62166238e-01,\n",
              "         2.24277690e-01,  9.44643974e-01, -3.07740942e-02,\n",
              "         6.73770189e-01,  6.14576995e-01,  9.71605718e-01,\n",
              "        -1.85494110e-01,  4.93627489e-01,  8.69804263e-01,\n",
              "        -6.64151728e-01, -2.68123507e-01, -6.08234107e-01,\n",
              "        -3.69645506e-02, -9.05510843e-01,  6.56540841e-02,\n",
              "        -9.54775333e-01,  9.56140101e-01,  7.75329173e-01,\n",
              "         3.08809459e-01,  1.51515037e-01,  4.47971970e-01,\n",
              "         1.00000000e+00,  1.68038145e-01,  6.59894347e-01,\n",
              "        -6.30283535e-01,  9.12612319e-01, -9.95692968e-01,\n",
              "        -8.52030396e-01, -3.34411412e-01, -1.20693422e-03,\n",
              "        -4.83690828e-01, -2.51822948e-01,  3.00235063e-01,\n",
              "        -9.66064751e-01,  5.35381794e-01,  3.74043941e-01,\n",
              "        -9.93562579e-01, -9.90300894e-01, -7.95615744e-03,\n",
              "         8.99479032e-01, -7.57308379e-02, -9.05740261e-01,\n",
              "        -7.31628418e-01, -5.70998132e-01,  5.11562228e-01,\n",
              "        -1.48406491e-01, -9.47974682e-01,  7.28678107e-02,\n",
              "        -2.15140358e-01,  4.59602147e-01, -1.03449851e-01,\n",
              "         5.07340908e-01,  5.25285006e-01,  7.68030643e-01,\n",
              "        -6.12814799e-02,  4.12747078e-03, -2.18543373e-02,\n",
              "        -8.33980203e-01,  8.48467350e-01, -8.22999358e-01,\n",
              "        -7.16393352e-01, -1.46959752e-01,  1.00000000e+00,\n",
              "        -4.85978186e-01,  6.13454759e-01,  7.68743157e-01,\n",
              "         7.83939958e-01, -4.51059937e-02,  9.10260230e-02,\n",
              "         7.63325095e-01,  1.97536647e-01, -4.33634400e-01,\n",
              "        -5.32926738e-01, -8.52954209e-01, -3.35147023e-01,\n",
              "         6.70421124e-01, -1.36236534e-01,  2.71684110e-01,\n",
              "         7.93170273e-01,  5.52096188e-01,  8.34109560e-02,\n",
              "         7.53918216e-02, -7.18323439e-02,  9.99655604e-01,\n",
              "        -2.36583352e-01, -1.02163568e-01, -4.50797617e-01,\n",
              "         8.56164321e-02, -2.99588561e-01, -6.27860308e-01,\n",
              "         1.00000000e+00,  3.09296578e-01,  1.41666800e-01,\n",
              "        -9.87675428e-01, -6.84573531e-01, -9.18106556e-01,\n",
              "         9.99979675e-01,  8.22207153e-01, -7.27703571e-01,\n",
              "         6.71166599e-01,  6.61144972e-01, -7.51447454e-02,\n",
              "         8.67583632e-01, -1.34572193e-01, -2.43492678e-01,\n",
              "         2.47773901e-01,  1.01436496e-01,  9.52548265e-01,\n",
              "        -4.74432111e-01, -9.59572196e-01, -5.93147874e-01,\n",
              "         3.37084860e-01, -9.55652952e-01,  9.97345984e-01,\n",
              "        -4.41248566e-01, -1.39766082e-01, -3.67007405e-01,\n",
              "         2.09465250e-01,  7.44721472e-01, -6.94671124e-02,\n",
              "        -9.80802298e-01, -9.72210988e-02,  1.44857168e-01,\n",
              "         9.58788931e-01,  1.93910643e-01, -4.98243779e-01,\n",
              "        -8.91368449e-01,  5.80144525e-01,  5.57696462e-01,\n",
              "        -6.85081661e-01, -9.49065149e-01,  9.60720658e-01,\n",
              "        -9.86090779e-01,  5.96757591e-01,  1.00000000e+00,\n",
              "         2.49726862e-01, -3.73955727e-01,  1.05005063e-01,\n",
              "        -4.44235325e-01,  2.61233479e-01, -2.30781898e-01,\n",
              "         7.12353885e-01, -9.56702948e-01, -3.18886787e-01,\n",
              "        -1.66044608e-01,  3.28748792e-01, -1.12054504e-01,\n",
              "        -3.69483698e-03,  7.02914000e-01,  1.76841930e-01,\n",
              "        -4.31119502e-01, -5.88681102e-01,  1.97730325e-02,\n",
              "         3.70398164e-01,  8.41151655e-01, -2.79781222e-01,\n",
              "        -9.03543308e-02,  8.15272406e-02, -9.31737125e-02,\n",
              "        -9.20582652e-01, -1.96440026e-01, -3.70618373e-01,\n",
              "        -9.99729156e-01,  6.25054300e-01, -1.00000000e+00,\n",
              "         1.06205039e-01, -1.22062907e-01, -1.67858571e-01,\n",
              "         8.43653321e-01, -3.79173383e-02,  3.50614816e-01,\n",
              "        -7.80137479e-01, -5.37364483e-01,  7.05084026e-01,\n",
              "         7.89132237e-01, -2.81294554e-01, -4.39532816e-01,\n",
              "        -7.05143452e-01,  2.41795063e-01, -6.60879118e-03,\n",
              "         1.87020525e-01, -4.42418486e-01,  7.71685839e-01,\n",
              "        -1.59949347e-01,  1.00000000e+00,  7.27321059e-02,\n",
              "        -6.92682803e-01, -9.80227172e-01,  1.45769775e-01,\n",
              "        -2.53912479e-01,  9.99998808e-01, -9.20311630e-01,\n",
              "        -9.46610451e-01,  3.11591953e-01, -6.46888912e-01,\n",
              "        -8.43633235e-01,  2.57953882e-01, -4.28976938e-02,\n",
              "        -7.77048290e-01, -8.45939696e-01,  9.53615248e-01,\n",
              "         9.24894392e-01, -4.93887037e-01,  3.99111897e-01,\n",
              "        -3.01058739e-01, -4.78791714e-01,  7.66606629e-02,\n",
              "         5.64246118e-01,  9.85007167e-01,  1.38763458e-01,\n",
              "         8.89845669e-01,  5.54780841e-01, -5.14287800e-02,\n",
              "         9.60235775e-01,  1.82936147e-01,  5.69802701e-01,\n",
              "         4.48231623e-02,  1.00000000e+00,  2.85030812e-01,\n",
              "        -9.18389022e-01,  2.67529607e-01, -9.78279710e-01,\n",
              "        -1.64021119e-01, -9.64396656e-01,  2.89010912e-01,\n",
              "         1.60544932e-01,  8.81933630e-01, -2.29691967e-01,\n",
              "         9.59473550e-01, -3.77213448e-01,  7.06327483e-02,\n",
              "        -6.05833232e-01, -4.47214097e-02,  3.69684577e-01,\n",
              "        -8.92718434e-01, -9.77248132e-01, -9.83738005e-01,\n",
              "         5.75852334e-01, -4.32877064e-01,  3.88910994e-04,\n",
              "         1.34517521e-01,  8.86910781e-02,  2.99988419e-01,\n",
              "         4.23277944e-01, -1.00000000e+00,  9.38144624e-01,\n",
              "         4.09939557e-01,  7.01781690e-01,  9.55278754e-01,\n",
              "         5.53572774e-01,  4.00907069e-01,  2.31861472e-01,\n",
              "        -9.79265213e-01, -9.85837162e-01, -2.86734074e-01,\n",
              "        -2.43372485e-01,  7.24926293e-01,  5.73148310e-01,\n",
              "         8.82385433e-01,  4.47218388e-01, -4.88187730e-01,\n",
              "         1.08281292e-01, -1.54022845e-02, -1.93978325e-01,\n",
              "        -9.91734862e-01,  4.41300184e-01, -3.44677150e-01,\n",
              "        -9.75851536e-01,  9.54579413e-01, -3.43957990e-01,\n",
              "        -7.95802474e-02,  2.57654667e-01, -4.46300715e-01,\n",
              "         9.62603092e-01,  7.00187862e-01,  4.77411091e-01,\n",
              "         4.40100431e-02,  4.43774402e-01,  8.69743049e-01,\n",
              "         9.54239666e-01,  9.83518422e-01, -4.93685603e-01,\n",
              "         7.92177260e-01, -3.51315349e-01,  3.86248112e-01,\n",
              "         3.85652423e-01, -9.28632021e-01,  7.07842484e-02,\n",
              "         1.62140369e-01, -3.15877438e-01,  2.04075634e-01,\n",
              "        -1.45915374e-01, -9.81760085e-01,  4.56553876e-01,\n",
              "        -2.50131428e-01,  5.35186768e-01, -3.01789820e-01,\n",
              "         1.20848075e-01, -3.81126016e-01, -1.13084570e-01,\n",
              "        -6.94658160e-01, -7.33376145e-01,  5.44230878e-01,\n",
              "         4.43657905e-01,  8.86685908e-01,  6.69312418e-01,\n",
              "        -5.67217879e-02, -7.12914526e-01, -2.42558971e-01,\n",
              "        -4.30712104e-01, -9.17453706e-01,  9.43061948e-01,\n",
              "        -9.84569639e-03, -3.45526002e-02,  2.27371514e-01,\n",
              "        -1.47409692e-01,  5.27877092e-01, -1.67280093e-01,\n",
              "        -3.80336106e-01, -3.09246898e-01, -6.63272440e-01,\n",
              "         8.34703267e-01, -4.29885387e-02, -4.60026592e-01,\n",
              "        -5.95243096e-01,  6.28581822e-01,  2.47767836e-01,\n",
              "         9.99393821e-01, -4.35214669e-01, -7.03353107e-01,\n",
              "        -1.45620793e-01, -3.09565336e-01,  2.78739780e-01,\n",
              "        -4.09911305e-01, -1.00000000e+00,  3.92048389e-01,\n",
              "        -1.81844562e-01,  4.28094804e-01, -5.48635662e-01,\n",
              "         3.98620039e-01, -5.09462953e-01, -9.82032537e-01,\n",
              "        -1.80749968e-01,  7.77132511e-02,  4.61079150e-01,\n",
              "        -4.80770916e-01, -6.77988410e-01,  4.47490513e-01,\n",
              "        -4.31427769e-02,  8.98085058e-01,  8.62030745e-01,\n",
              "        -1.49507850e-01,  4.16305751e-01,  5.60364842e-01,\n",
              "        -3.05751652e-01, -6.40093744e-01,  9.16617632e-01]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TzlCjPZmwRZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}