{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_nlp_topic_modeling.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaz-ncr/100ml/blob/master/10_nlp_ticket_text_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjJG2en2AxgM"
      },
      "source": [
        "doc1 = \"\"\"BJP leader Poonam Mahajan said Tuesday the party’s youth wing will challenge the Supreme Court order asking its activist Priyanka Sharma to apologise for posting a meme of West Bengal CM Mamata Banerjee on social media, asserting the verdict has sent out a “wrong message”.\n",
        "\n",
        "The court earlier Tuesday granted bail to Sharma, against whom a complaint was filed in Bengal for sharing a photo in which Banerjee’s face was photoshopped onto actor Priyanka Chopra’s picture from a MET Gala event in New York.\n",
        "\n",
        "The apex court also asked Sharma to apologise in writing on her release from jail. It observed freedom of speech ends when it infringes upon the rights of others.\n",
        "\n",
        "Mahajan said she was “delighted” with the SC order and “grateful” to the court for releasing Sharma on bail.\n",
        "\n",
        "“But the direction of apology sends out a wrong message on the question of free speech,” she said.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "doc2 = \"\"\"New Delhi: The monsoon is likely to be delayed this year as its arrival expected on June 6, five days after its normal onset date, the India Meteorological Department said on Wednesday. \"This year, the statistical model forecast suggests that the monsoon onset over Kerala is likely to be slightly delayed,\" the IMD said. \n",
        "\n",
        "\"The southwest monsoon onset is likely to set over Kerala on 6th June with a model error of plus or minus 4 days.\"\n",
        "\n",
        "\n",
        "If the monsoon arrives late, it will be third such instance since 2014 when it arrived on June 5, followed by June 6 in 2015 and June 8 in 2016.\n",
        "\n",
        "The delay in the arrival of monsoon may not necessarily have an impact on the overall rainfall. Last year, it had hit Kerala on May 29, three days before the normal onset date. Yet, the country received 'below-normal' rainfall.\n",
        "\n",
        "Similarly, in 2017, the monsoon arrived in Kerala on May 30, but the overall rainfall was 95 per cent of the long period average (LPA), which falls under the below normal category.\n",
        "\n",
        "In its initial forecast released in April, the IMD had predicted a near-normal rainfall with an LPA of 96 per cent, which falls on the border of 'below-normal' and 'normal' rainfall category. On the other hand, the Skymet has predicted a 'below-normal' rainfall with an LPA of 93 per cent.\"\"\"\n",
        "doc_complete = [doc1, doc2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqjC-JlKB2oZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a528e008-5936-416b-801c-9f2e4f102eb2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation) \n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete]    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOKXG7eyCy0I"
      },
      "source": [
        "doc_clean\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRG8mlHWDd5X"
      },
      "source": [
        "# Importing Gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
        "\n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0qZWhYzD0QK"
      },
      "source": [
        "dictionary\n",
        "doc_term_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u43N1382D7Db"
      },
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Trainign LDA model on the document term matrix.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5OkPm9ECET",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d3eb192f-f39a-41b9-edc0-66f45f61a06d"
      },
      "source": [
        "print(ldamodel.print_topics(num_topics=5, num_words=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.036*\"monsoon\" + 0.026*\"june\" + 0.026*\"rainfall\" + 0.022*\"kerala\" + 0.019*\"onset\"'), (1, '0.004*\"may\" + 0.004*\"also\" + 0.004*\"new\" + 0.004*\"year\" + 0.004*\"two\"'), (2, '0.021*\"bengal\" + 0.018*\"said\" + 0.018*\"sharma\" + 0.014*\"court\" + 0.014*\"mahajan\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}